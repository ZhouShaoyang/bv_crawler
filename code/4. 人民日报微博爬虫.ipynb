{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人民日报微博抓取示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 基础准备（第三方库的安装，标准库与第三方库的引用，目标URL和请求报文头的初始化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautifulsoup库，lxml库，tqdm库并非python标准库，因此需要自行安装，可使用pip命令快速安装\n",
    "# 使用pip工具安装beautifulsoup库，lxml库，tqdm库（这里我们引入清华pypi镜像站以提高源码下载速度）\n",
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple beautifulsoup4 lxml tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入requests库，用于处理发送请求与接收响应\n",
    "import requests\n",
    "# 引入beautifulsoup库，用于处理html网页解析\n",
    "from bs4 import BeautifulSoup\n",
    "# 引入re库，用于匹配文本\n",
    "import re\n",
    "# 引入time包，用于控制请求的频率\n",
    "import time\n",
    "# 引入tqdm包，用于监控抓取进度\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义抓取人民日报微博帖子的初始URL (%d为整数占位符，可以代表一个整数数字)\n",
    "begin_url = 'https://weibo.cn/rmrb?page=%d'\n",
    "\n",
    "# 定义抓取的全部网页URL（抓取N页）\n",
    "N = 50                          # 设定抓取50页\n",
    "page_list = []                  # 定义空列表，用于存储全部网页URL\n",
    "for item in range(1, N + 1):    # 循环遍历1到N+1\n",
    "    url = begin_url % item      # 拼接单页URL\n",
    "    page_list.append(url)       # 把单页URL添加到page_list列表内\n",
    "    \n",
    "# 构造请求报文头，weibo.cn站点访问的报文请求头需要至少添加账号cookie和用户代理user-agent\n",
    "# 这两个信息在登陆微博后可以使用chrome浏览器的调试模式F12，NetWork -> Headers -> Requests Headers中找到，复制到对应变量内即可\n",
    "cookie = '_T_WM=e875e1c9226638113f452f97bde99e47; SCF=AtrTOj7AMx8A58Sqgh7eoC9IoltPjeHcdQG65VL03DQyJJqxIlyL-3g_gALnbwYqqfXHSO48wrfPw6kx8cdX5P8.; SUB=_2A25zl6_XDeRhGeBK6lsW8ijKzT2IHXVRezGfrDV6PUJbkdANLVmhkW1NR_ZNnoEO33Z9EvtsXEC2a1tD2pkMomTa; SUHB=0LcSFl6HDcU8UG; SSOLoginState=1586749319'\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'\n",
    "headers = {'cookie': cookie, 'user-agent': user_agent}  # 定义字典，用于存储报文请求头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印全部网页URL\n",
    "print(page_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印请求报文头\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 演示单页网页的数据抓取及清洗过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 取出page_list中的第一页\n",
    "sample_url = page_list[0]       # sample_url = https://weibo.cn/rmrb?page=1\n",
    "# 打印第一页url\n",
    "print(sample_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 使用requests包发送请求，并接收响应\n",
    "sample_response_body = requests.get(url=sample_url, headers=headers).text\n",
    "# 打印html网页文本\n",
    "print(sample_response_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 使用beautifulsoup包解析html网页文本（使用lxml解析器）\n",
    "sample_beautifulsoup = BeautifulSoup(sample_response_body, 'lxml')\n",
    "# 打印解析后的html对象(实际上与上面的文本一致，但是经过beautifulsoup解析后的文本是可以依据标签提取内容的)\n",
    "print(sample_beautifulsoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 获取所有包含目标文本的div元素列表\n",
    "sample_div_list = sample_beautifulsoup.find_all(attrs={\"class\": \"c\", \"id\": re.compile(r'M_I.*?')})\n",
    "# 打印提取的div块级元素\n",
    "print(sample_div_list)\n",
    "# 打印提取的div块级元素数量\n",
    "print(len(sample_div_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 遍历div_list，获取目标文本并适当的文本清洗\n",
    "for sample_div in sample_div_list:\n",
    "    sample_post = sample_div.text\n",
    "    sample_clean_post = re.sub(re.compile(r'赞.*?转发.*?评论.*?收藏.*?$'), '', sample_post)  # 清除'赞[]转发[]评论[]收藏...'内容\n",
    "    sample_clean_post = re.sub(re.compile(r'\\[组图共\\d张\\]'), '', sample_clean_post)        # 清除'组图共n张'内容\n",
    "    sample_clean_post = sample_clean_post.replace('原图', '').replace('...全文', '')        # 清除'原图'，'...全文'内容\n",
    "    # 打印文本清洗结果\n",
    "    print(sample_div_list.index(sample_div)+1, sample_clean_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 完整抓取全部网页的目标数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正式的循环抓取过程，使用tqdm库监控抓取进度\n",
    "with open('人民日报微博语料.txt', 'w', encoding='utf-8') as file:\n",
    "    for url in tqdm(page_list):\n",
    "        response_body = requests.get(url=url, headers=headers).text\n",
    "        # 定义每次请求后等待3秒\n",
    "        time.sleep(3)\n",
    "        beautifulsoup = BeautifulSoup(response_body, 'lxml')\n",
    "        div_list = beautifulsoup.find_all(attrs={\"class\": \"c\", \"id\": re.compile(r'M_I.*?')})\n",
    "        for div in div_list:\n",
    "            post = div.text\n",
    "            clean_post = re.sub(re.compile(r'赞.*?转发.*?评论.*?收藏.*?$'), '', post)\n",
    "            clean_post = re.sub(re.compile(r'\\[组图共\\d张\\]'), '', clean_post)\n",
    "            clean_post = clean_post.replace('原图', '').replace('...全文', '')\n",
    "            # 把清洗过的帖子写入上面打开的weibo_post.txt文件内\n",
    "            file.write('%s\\n' % clean_post)\n",
    "print('抓取完毕')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
